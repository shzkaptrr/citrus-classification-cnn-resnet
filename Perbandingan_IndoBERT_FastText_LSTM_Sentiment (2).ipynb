{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Proyek ini bertujuan untuk mengklasifikasikan sentimen (0: Negatif, 1: Positif) dari 10.000 ulasan aplikasi MyIM3. Secara kritis, proyek ini menguji hipotesis bahwa model Transformer (IndoBERT) yang sudah pre-trained akan mengungguli model sekuensial tradisional (LSTM), baik yang dilatih from scratch (dari nol) maupun yang diperkuat dengan pre-trained word embedding (FastText).\n"
      ],
      "metadata": {
        "id": "scq6EEkPKCUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLDiu3-1MRYa",
        "outputId": "9e630878-520e-408c-db49-0317fcf00f71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dataset & Praproses"
      ],
      "metadata": {
        "id": "DnxSja5xDoWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8apmSHtCJhy",
        "outputId": "b3f2bae4-6497-4560-db25-d706af85a0d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ucupsedaya/10k-myim3-app-reviews?dataset_version_number=1&file_name=myIM3_Beli_Pulsa__Cek_Kuota.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 411k/411k [00:00<00:00, 578kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting zip of myIM3_Beli_Pulsa__Cek_Kuota.csv...\n",
            "✅ Dataset berhasil dimuat dari Kaggle!\n",
            "                userName                                            content  \\\n",
            "0   UCIL OLOL LEHO [UOL]  Kenapa Indosat ada tuyulnya sekarang pulsa tib...   \n",
            "1              Pp.joaana                          aplikasi ini sangat bagus   \n",
            "2              Xeraphine  Tolong itu sistem login nomor utama dan nomor ...   \n",
            "3  Sugeng dian pamungkas  Sekarang pilihanya yg 30 hari naik terus harga...   \n",
            "4             Bang Tinus                                              murah   \n",
            "\n",
            "   score                   at appVersion  \n",
            "0      1  2024-02-02 05:17:07     82.0.5  \n",
            "1      5  2024-02-02 05:16:51     82.0.5  \n",
            "2      3  2024-02-02 05:13:29        NaN  \n",
            "3      4  2024-02-02 05:13:07        NaN  \n",
            "4      5  2024-02-02 05:11:20    81.16.0  \n",
            "✅ Data bersih siap: 10000 baris.\n",
            "Distribusi label:\n",
            " label\n",
            "0    5123\n",
            "1    4877\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# 1️⃣ Unduh dataset langsung dari Kaggle\n",
        "# (file_path diisi dengan nama file di dalam dataset)\n",
        "file_path = \"myIM3_Beli_Pulsa__Cek_Kuota.csv\"\n",
        "\n",
        "df = kagglehub.dataset_load(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"ucupsedaya/10k-myim3-app-reviews\",\n",
        "    file_path\n",
        ")\n",
        "\n",
        "print(\"✅ Dataset berhasil dimuat dari Kaggle!\")\n",
        "print(df.head())\n",
        "\n",
        "# 2️⃣ Rename kolom agar konsisten\n",
        "df = df.rename(columns={'content': 'Text Review', 'score': 'Rating'}, errors='ignore')\n",
        "\n",
        "# 3️⃣ Fungsi pembersihan teks\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', str(text))\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'RT\\s', '', text)\n",
        "    text = re.sub(r'([.!?])\\1+', r'\\1', text)\n",
        "    text = re.sub(r'[\\n\\t]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# 4️⃣ Bersihkan teks\n",
        "df['cleaned_text'] = df['Text Review'].apply(clean_text)\n",
        "\n",
        "# 5️⃣ Coba tidak hapus yang netral | Konversi rating ke label biner (hapus yang netral)\n",
        "def rating_to_binary_label(rating):\n",
        "    if rating >= 3:\n",
        "        return 1\n",
        "    elif rating <= 2:\n",
        "        return 0\n",
        "    return None\n",
        "\n",
        "df['label'] = df['Rating'].apply(rating_to_binary_label)\n",
        "df.dropna(subset=['label'], inplace=True)\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# 6️⃣ Pilih kolom akhir\n",
        "df = df.rename(columns={'Text Review': 'Text Tweet'}, errors='ignore')\n",
        "df = df[['cleaned_text', 'label']]\n",
        "\n",
        "# 7️⃣ Cek hasil akhir\n",
        "print(f\"✅ Data bersih siap: {len(df)} baris.\")\n",
        "print(\"Distribusi label:\\n\", df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Dataset"
      ],
      "metadata": {
        "id": "P-fFccddkRLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Split Dataset\n",
        "# Pisahkan 20% data untuk Test Set\n",
        "df_train_val, df_test = train_test_split(df,\n",
        "                                         test_size=0.2,\n",
        "                                         random_state=42,\n",
        "                                         stratify=df['label'])\n",
        "\n",
        "# Bagi sisa data menjadi Train dan Validation\n",
        "df_train, df_val = train_test_split(df_train_val,\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=42,\n",
        "                                    stratify=df_train_val['label'])\n",
        "\n",
        "print(f\"Jumlah data training: {len(df_train)}\")\n",
        "print(f\"Jumlah data validasi: {len(df_val)}\")\n",
        "print(f\"Jumlah data test: {len(df_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yYrBZXvkQsX",
        "outputId": "43d8b10f-a767-47b1-9820-93cb3c21560c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data training: 6400\n",
            "Jumlah data validasi: 1600\n",
            "Jumlah data test: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indobert"
      ],
      "metadata": {
        "id": "iM7Y-hyqZFa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisasi"
      ],
      "metadata": {
        "id": "y4TIffIRSH6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Tokenisasi (Transformer)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "NAMA_MODEL = \"indobenchmark/indobert-base-p2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(NAMA_MODEL)\n",
        "MAX_LEN = 128\n",
        "\n",
        "train_encodings = tokenizer(df_train['cleaned_text'].tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
        "val_encodings = tokenizer(df_val['cleaned_text'].tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
        "test_encodings_trans = tokenizer(df_test['cleaned_text'].tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
        "\n",
        "print(\"✅ Tokenisasi Transformer selesai.\")"
      ],
      "metadata": {
        "id": "O1Mc9RjoSOcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membuat Dataset (PyTorch)"
      ],
      "metadata": {
        "id": "CmaGG6jnSofV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Dataset PyTorch & Metrik\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Definisikan Class Dataset\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Buat dataset untuk Transformer\n",
        "train_dataset = SentimentDataset(train_encodings, df_train['label'].tolist())\n",
        "val_dataset = SentimentDataset(val_encodings, df_val['label'].tolist())\n",
        "test_dataset_trans = SentimentDataset(test_encodings_trans, df_test['label'].tolist())\n",
        "\n",
        "# Definisikan fungsi untuk menghitung metrik\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    precision = precision_score(labels, predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(labels, predictions, average='macro', zero_division=0)\n",
        "    f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "print(\"✅ Dataset dan Fungsi Metrik siap.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCHFdJoITcgH",
        "outputId": "c9df5159-cb88-4716-84bb-64e52558e555"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset dan Fungsi Metrik siap.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Definisikan fungsi untuk menghitung metrik\n",
        "def compute_metrics(eval_pred):\n",
        "    # 'eval_pred' adalah tuple berisi (predictions, labels)\n",
        "\n",
        "    # 1. Ambil logits dan label aslinya\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # 2. Ubah logits menjadi tebakan (0 atau 1)\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # 3. Hitung metrik menggunakan sklearn (macro average)\n",
        "    precision = precision_score(labels, predictions, average='macro')\n",
        "    recall = recall_score(labels, predictions, average='macro')\n",
        "    f1 = f1_score(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    # 4. Kembalikan dalam bentuk dictionary\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "print(\"✅ Fungsi compute_metrics siap.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jknLFnE4IV4Q",
        "outputId": "b6c5eabe-296c-4d66-b5c4-4646d855bd36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fungsi compute_metrics siap.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model (Fine-Tuning) - Transformer"
      ],
      "metadata": {
        "id": "8fKD9fxPSrFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Training Model Transformer\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "# ✨ TAMBAHAN: Import library ini untuk mematikan bar download\n",
        "from huggingface_hub import utils\n",
        "\n",
        "# ✨ TAMBAHAN: Matikan progress bar download\n",
        "utils.disable_progress_bars()\n",
        "\n",
        "NAMA_MODEL = \"indobenchmark/indobert-base-p2\"\n",
        "\n",
        "# Bar download (hijau) tidak akan muncul lagi setelah baris di atas\n",
        "model_trans = AutoModelForSequenceClassification.from_pretrained(NAMA_MODEL, num_labels=2)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_myim3_trans',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_myim3_trans',\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    disable_tqdm=True # Ini mematikan bar saat training (looping)\n",
        ")\n",
        "\n",
        "trainer_trans = Trainer(\n",
        "    model=model_trans,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Memulai Training Transformer (IndoBERT)...\")\n",
        "trainer_trans.train()\n",
        "print(\"✅ Training Transformer selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJnfnKsCTpQu",
        "outputId": "3883b0ea-98c0-4124-aabe-b2c91e592658"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai Training Transformer (IndoBERT)...\n",
            "{'loss': 0.5934, 'grad_norm': 4.419984817504883, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.125}\n",
            "{'loss': 0.3426, 'grad_norm': 2.623445987701416, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.25}\n",
            "{'loss': 0.3513, 'grad_norm': 2.0583078861236572, 'learning_rate': 1.49e-05, 'epoch': 0.375}\n",
            "{'loss': 0.3353, 'grad_norm': 4.750554084777832, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.5}\n",
            "{'loss': 0.3315, 'grad_norm': 3.976949691772461, 'learning_rate': 2.4900000000000002e-05, 'epoch': 0.625}\n",
            "{'loss': 0.3052, 'grad_norm': 2.7730374336242676, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.75}\n",
            "{'loss': 0.3456, 'grad_norm': 2.2314631938934326, 'learning_rate': 3.49e-05, 'epoch': 0.875}\n",
            "{'loss': 0.3332, 'grad_norm': 19.415319442749023, 'learning_rate': 3.99e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.3543838560581207, 'eval_accuracy': 0.895, 'eval_f1': 0.8947234846526579, 'eval_precision': 0.8964122007809405, 'eval_recall': 0.8942776735459663, 'eval_runtime': 11.0128, 'eval_samples_per_second': 145.285, 'eval_steps_per_second': 2.27, 'epoch': 1.0}\n",
            "{'loss': 0.327, 'grad_norm': 2.4319236278533936, 'learning_rate': 4.49e-05, 'epoch': 1.125}\n",
            "{'loss': 0.3475, 'grad_norm': 2.207265853881836, 'learning_rate': 4.99e-05, 'epoch': 1.25}\n",
            "{'loss': 0.296, 'grad_norm': 6.02291202545166, 'learning_rate': 4.6500000000000005e-05, 'epoch': 1.375}\n",
            "{'loss': 0.296, 'grad_norm': 4.52197790145874, 'learning_rate': 4.292857142857143e-05, 'epoch': 1.5}\n",
            "{'loss': 0.2912, 'grad_norm': 2.917848587036133, 'learning_rate': 3.935714285714286e-05, 'epoch': 1.625}\n",
            "{'loss': 0.3222, 'grad_norm': 4.3847761154174805, 'learning_rate': 3.5785714285714286e-05, 'epoch': 1.75}\n",
            "{'loss': 0.3134, 'grad_norm': 7.093858242034912, 'learning_rate': 3.221428571428571e-05, 'epoch': 1.875}\n",
            "{'loss': 0.2908, 'grad_norm': 5.790517807006836, 'learning_rate': 2.8642857142857144e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.3245421350002289, 'eval_accuracy': 0.895625, 'eval_f1': 0.8956034274269956, 'eval_precision': 0.8955836878956109, 'eval_recall': 0.8958255159474672, 'eval_runtime': 11.136, 'eval_samples_per_second': 143.678, 'eval_steps_per_second': 2.245, 'epoch': 2.0}\n",
            "{'loss': 0.225, 'grad_norm': 4.036834716796875, 'learning_rate': 2.5071428571428574e-05, 'epoch': 2.125}\n",
            "{'loss': 0.2747, 'grad_norm': 1.8966434001922607, 'learning_rate': 2.15e-05, 'epoch': 2.25}\n",
            "{'loss': 0.2071, 'grad_norm': 2.2776858806610107, 'learning_rate': 1.792857142857143e-05, 'epoch': 2.375}\n",
            "{'loss': 0.2447, 'grad_norm': 4.084017753601074, 'learning_rate': 1.4357142857142858e-05, 'epoch': 2.5}\n",
            "{'loss': 0.2326, 'grad_norm': 0.9889411926269531, 'learning_rate': 1.0785714285714287e-05, 'epoch': 2.625}\n",
            "{'loss': 0.2009, 'grad_norm': 2.308164358139038, 'learning_rate': 7.214285714285715e-06, 'epoch': 2.75}\n",
            "{'loss': 0.1755, 'grad_norm': 1.9855290651321411, 'learning_rate': 3.642857142857143e-06, 'epoch': 2.875}\n",
            "{'loss': 0.2624, 'grad_norm': 1.7040386199951172, 'learning_rate': 7.142857142857144e-08, 'epoch': 3.0}\n",
            "{'eval_loss': 0.3533703684806824, 'eval_accuracy': 0.89875, 'eval_f1': 0.8986074166797058, 'eval_precision': 0.8991228070175439, 'eval_recall': 0.8983739837398375, 'eval_runtime': 11.1618, 'eval_samples_per_second': 143.346, 'eval_steps_per_second': 2.24, 'epoch': 3.0}\n",
            "{'train_runtime': 518.6086, 'train_samples_per_second': 37.022, 'train_steps_per_second': 2.314, 'train_loss': 0.3018778387705485, 'epoch': 3.0}\n",
            "✅ Training Transformer selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi Model - Transformer"
      ],
      "metadata": {
        "id": "I3zfY6QPStcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Evaluasi Model Transformer\n",
        "print(\"Memulai evaluasi final Transformer pada Test Set...\")\n",
        "test_results_trans = trainer_trans.evaluate(eval_dataset=test_dataset_trans)\n",
        "\n",
        "print(\"\\n--- Hasil Evaluasi Test Set Transformer (IndoBERT) ---\")\n",
        "print(test_results_trans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zNygWvQTtxN",
        "outputId": "a1f21b25-7a7b-4dad-9d68-dda27dde8470"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai evaluasi final Transformer pada Test Set...\n",
            "{'eval_loss': 0.335831880569458, 'eval_accuracy': 0.8845, 'eval_f1': 0.8844560644184951, 'eval_precision': 0.8844003424671236, 'eval_recall': 0.8845653533458411, 'eval_runtime': 12.5841, 'eval_samples_per_second': 158.93, 'eval_steps_per_second': 2.543, 'epoch': 3.0}\n",
            "\n",
            "--- Hasil Evaluasi Test Set Transformer (IndoBERT) ---\n",
            "{'eval_loss': 0.335831880569458, 'eval_accuracy': 0.8845, 'eval_f1': 0.8844560644184951, 'eval_precision': 0.8844003424671236, 'eval_recall': 0.8845653533458411, 'eval_runtime': 12.5841, 'eval_samples_per_second': 158.93, 'eval_steps_per_second': 2.543, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Dapatkan prediksi mentah (logits) dari model Transformer (IndoBERT)\n",
        "print(\"Memulai prediksi dengan model Transformer (IndoBERT) pada Test Set...\")\n",
        "pred_output = trainer_trans.predict(test_dataset_trans)\n",
        "\n",
        "# 2. Ambil logits dan label aslinya\n",
        "logits = pred_output.predictions\n",
        "y_true_bert = pred_output.label_ids\n",
        "\n",
        "# 3. Ubah logits menjadi label (0 atau 1), sama seperti di compute_metrics\n",
        "y_pred_bert = np.argmax(logits, axis=1)\n",
        "\n",
        "# 4. Tampilkan classification_report\n",
        "print(\"\\n--- Hasil Evaluasi Test Set Transformer (IndoBERT) [Format Laporan] ---\")\n",
        "print(classification_report(\n",
        "    y_true_bert,\n",
        "    y_pred_bert,\n",
        "    target_names=['Negative (0)', 'Positive (1)'], # Pastikan urutannya benar\n",
        "    zero_division=0\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QouydiAqwt78",
        "outputId": "7495a76a-3021-4695-ab60-3e8685a1e7ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai prediksi dengan model Transformer (IndoBERT) pada Test Set...\n",
            "\n",
            "--- Hasil Evaluasi Test Set Transformer (IndoBERT) [Format Laporan] ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Negative (0)       0.89      0.88      0.89      1025\n",
            "Positive (1)       0.88      0.89      0.88       975\n",
            "\n",
            "    accuracy                           0.88      2000\n",
            "   macro avg       0.88      0.88      0.88      2000\n",
            "weighted avg       0.88      0.88      0.88      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "O4kH2PMyZMpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisasi & Sequence LSTM"
      ],
      "metadata": {
        "id": "9XYnvL0jlMbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 Tokenisasi & Sequence (LSTM)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Parameter untuk Tokenizer LSTM\n",
        "LSTM_MAX_LEN = 128\n",
        "MAX_WORDS = 20000\n",
        "\n",
        "# --- INI ADALAH PERBAIKANNYA ---\n",
        "# Kita definisikan filter default Keras, tapi KITA HAPUS \"0123456789\" dari daftarnya\n",
        "# sehingga angka tetap aman.\n",
        "custom_filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "# 1. Inisialisasi tokenizer DENGAN FILTER BARU\n",
        "tokenizer_lstm = Tokenizer(\n",
        "    num_words=MAX_WORDS,\n",
        "    oov_token=\"<oov>\",\n",
        "    filters=custom_filters, # <-- Tambahkan baris ini\n",
        "    lower=True # <-- Pastikan ini True (memang default-nya)\n",
        ")\n",
        "\n",
        "# 2. Inisialisasi dan fit tokenizer\n",
        "tokenizer_lstm.fit_on_texts(df_train['cleaned_text'].tolist())\n",
        "\n",
        "# 3. Ubah teks menjadi sequence integer\n",
        "train_sequences = tokenizer_lstm.texts_to_sequences(df_train['cleaned_text'].tolist())\n",
        "val_sequences = tokenizer_lstm.texts_to_sequences(df_val['cleaned_text'].tolist())\n",
        "test_sequences = tokenizer_lstm.texts_to_sequences(df_test['cleaned_text'].tolist())\n",
        "\n",
        "# 4. Padding sequences\n",
        "X_train_lstm = pad_sequences(train_sequences, maxlen=LSTM_MAX_LEN, padding='post', truncating='post')\n",
        "X_val_lstm = pad_sequences(val_sequences, maxlen=LSTM_MAX_LEN, padding='post', truncating='post')\n",
        "X_test_lstm = pad_sequences(test_sequences, maxlen=LSTM_MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "# 5. Ambil label\n",
        "y_train_lstm = np.array(df_train['label'].tolist())\n",
        "y_val_lstm = np.array(df_val['label'].tolist())\n",
        "y_test_lstm = np.array(df_test['label'].tolist())\n",
        "\n",
        "print(\"✅ Tokenisasi dan Padding LSTM (dengan filter angka) selesai.\")\n",
        "print(f\"Contoh kata di kamus: 'myim3' ada di indeks {tokenizer_lstm.word_index.get('myim3')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMs4dAQ7lQ_R",
        "outputId": "6d9c8c41-c1ae-44a1-f479-5cc53994c0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tokenisasi dan Padding LSTM (dengan filter angka) selesai.\n",
            "Contoh kata di kamus: 'myim3' ada di indeks 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model LSTM & Training"
      ],
      "metadata": {
        "id": "6R8cKPsAlTgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Model LSTM dan Training\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2 # Import l2 regularizer\n",
        "\n",
        "# Model Hyperparameters\n",
        "EMBEDDING_DIM = 128\n",
        "VOCAB_SIZE = MAX_WORDS\n",
        "LSTM_UNITS = 16 # Sesuaikan dengan model sederhana\n",
        "\n",
        "# 1. Definisikan Model LSTM (Sudah disederhanakan)\n",
        "model_lstm = Sequential([ # Rename model_lstm_simple to model_lstm\n",
        "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=LSTM_MAX_LEN),\n",
        "    Bidirectional(LSTM(LSTM_UNITS, # Gunakan LSTM_UNITS yang baru\n",
        "                       kernel_regularizer=l2(0.01))), # Cukup satu layer Bi-LSTM\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Langsung ke output\n",
        "])\n",
        "\n",
        "# Definisikan callback-nya\n",
        "early_stop = EarlyStopping(monitor='val_loss', # Pantau loss validasi\n",
        "                           patience=2,       # Berhenti jika 2 epoch tidak membaik\n",
        "                           restore_best_weights=True) # Kembalikan ke bobot terbaik\n",
        "\n",
        "# 2. Compile Model (Gunakan NAMA YANG BENAR)\n",
        "model_lstm.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_lstm.summary() # Tampilkan summary dari model yang benar\n",
        "\n",
        "# 3. Training Model (Gunakan NAMA YANG BENAR)\n",
        "print(\"\\nMemulai Training Model LSTM...\")\n",
        "history_lstm = model_lstm.fit( # Gunakan model_lstm\n",
        "    X_train_lstm, y_train_lstm,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_lstm, y_val_lstm),\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "print(\"✅ Training LSTM selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "1vOy8WoOlV_X",
        "outputId": "b186ba2d-6854-4627-fe31-3bad0ed3a5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_21\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_21\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_23 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_20                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_42 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_20                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Memulai Training Model LSTM...\n",
            "Epoch 1/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - accuracy: 0.7199 - loss: 1.4664 - val_accuracy: 0.8325 - val_loss: 0.4538\n",
            "Epoch 2/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.8496 - loss: 0.4304 - val_accuracy: 0.8675 - val_loss: 0.3665\n",
            "Epoch 3/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.8900 - loss: 0.3466 - val_accuracy: 0.8737 - val_loss: 0.3492\n",
            "Epoch 4/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.9124 - loss: 0.2999 - val_accuracy: 0.8750 - val_loss: 0.3670\n",
            "Epoch 5/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.9248 - loss: 0.2642 - val_accuracy: 0.8838 - val_loss: 0.3773\n",
            "✅ Training LSTM selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi Model"
      ],
      "metadata": {
        "id": "gCLABOa4lYmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Evaluasi Model LSTM\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Memulai evaluasi final LSTM pada Test Set...\")\n",
        "\n",
        "# 1. Prediksi Probability\n",
        "y_pred_proba_lstm = model_lstm.predict(X_test_lstm, verbose=0)\n",
        "\n",
        "# 2. Konversi ke Label Biner (0 atau 1)\n",
        "y_pred_lstm = (y_pred_proba_lstm > 0.5).astype(int)\n",
        "\n",
        "# 3. Hitung Metrik\n",
        "report_lstm = classification_report(y_test_lstm, y_pred_lstm,\n",
        "                                    target_names=['Negative (0)', 'Positive (1)'],\n",
        "                                    output_dict=True, zero_division=0)\n",
        "\n",
        "print(\"\\n--- Hasil Evaluasi Test Set LSTM ---\")\n",
        "print(classification_report(y_test_lstm, y_pred_lstm,\n",
        "                            target_names=['Negative (0)', 'Positive (1)'],\n",
        "                            zero_division=0))\n",
        "\n",
        "# Ekstrak F1 Macro dan Accuracy\n",
        "acc_lstm = report_lstm['accuracy']\n",
        "f1_lstm = report_lstm['macro avg']['f1-score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aQohb3clcA8",
        "outputId": "efb846cb-03fa-4809-b6f5-5e7320ae6193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai evaluasi final LSTM pada Test Set...\n",
            "\n",
            "--- Hasil Evaluasi Test Set LSTM ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Negative (0)       0.88      0.87      0.87      1025\n",
            "Positive (1)       0.87      0.87      0.87       975\n",
            "\n",
            "    accuracy                           0.87      2000\n",
            "   macro avg       0.87      0.87      0.87      2000\n",
            "weighted avg       0.87      0.87      0.87      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM + FastText(cc.id)"
      ],
      "metadata": {
        "id": "oBvM_KprZXWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unduh Pre-trained Embedding (FastText cc.id)"
      ],
      "metadata": {
        "id": "rxMH8z4_Znte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11.A: Unduh Pre-trained Embedding (FastText cc.id)\n",
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# URL dan nama file\n",
        "url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz\"\n",
        "gz_filename = \"cc.id.300.vec.gz\"\n",
        "vec_filename = \"cc.id.300.vec\"\n",
        "\n",
        "# Fungsi untuk menampilkan progress bar\n",
        "def download_with_progress(url, filename):\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        total_size = int(r.headers.get('content-length', 0))\n",
        "        block_size = 8192\n",
        "        with open(filename, 'wb') as f:\n",
        "            with tqdm(total=total_size, unit='iB', unit_scale=True, desc=filename) as pbar:\n",
        "                for chunk in r.iter_content(chunk_size=block_size):\n",
        "                    pbar.update(len(chunk))\n",
        "                    f.write(chunk)\n",
        "\n",
        "try:\n",
        "    if os.path.exists(vec_filename):\n",
        "        print(f\"✅ File '{vec_filename}' sudah ada. Tidak perlu mengunduh lagi.\")\n",
        "    else:\n",
        "        # 1. Mengunduh file\n",
        "        print(f\"📥 Mengunduh file dari {url}...\")\n",
        "        if not os.path.exists(gz_filename):\n",
        "            download_with_progress(url, gz_filename)\n",
        "\n",
        "        # 2. Mengekstrak file .gz\n",
        "        print(f\"⚙️ Mengekstrak '{gz_filename}'... (Ini mungkin butuh beberapa menit)\")\n",
        "        with gzip.open(gz_filename, 'rb') as f_in:\n",
        "            with open(vec_filename, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "        print(f\"✅ Ekstraksi selesai. File '{vec_filename}' telah dibuat.\")\n",
        "\n",
        "        # 3. Hapus file .gz\n",
        "        os.remove(gz_filename)\n",
        "        print(f\"🗑️ File '{gz_filename}' telah dihapus untuk menghemat ruang.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Terjadi kesalahan: {e}\")"
      ],
      "metadata": {
        "id": "47ameV7hZkYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Matrix"
      ],
      "metadata": {
        "id": "ulAb20x9ZxJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11.B: Siapkan Embedding Matrix\n",
        "\n",
        "print(\"📖 Memuat pre-trained word vectors dari cc.id.300.vec...\")\n",
        "embeddings_index = {}\n",
        "try:\n",
        "    with open(vec_filename, 'r', encoding='utf-8') as f:\n",
        "        # Lewati baris header (jika ada, file cc.id biasanya ada)\n",
        "        next(f)\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            try:\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = coefs\n",
        "            except ValueError:\n",
        "                pass # Abaikan baris yang bermasalah\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: File '{vec_filename}' tidak ditemukan. Jalankan Cell 11.A dulu.\")\n",
        "    # Hentikan eksekusi jika file tidak ada\n",
        "    raise\n",
        "\n",
        "print(f'✅ Ditemukan {len(embeddings_index)} word vectors.')\n",
        "\n",
        "# --- Siapkan Matrix ---\n",
        "# Gunakan variabel dari Cell 9 & 10\n",
        "EMBEDDING_DIM_FT = 300 # Ini harus 300 sesuai nama file\n",
        "# Pastikan VOCAB_SIZE sama dengan yang di-set di Cell 10 (yaitu MAX_WORDS)\n",
        "VOCAB_SIZE = MAX_WORDS\n",
        "\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM_FT))\n",
        "\n",
        "# tokenizer_lstm diambil dari Cell 9\n",
        "for word, i in tokenizer_lstm.word_index.items():\n",
        "    if i >= VOCAB_SIZE:\n",
        "        continue # Hanya proses kata dalam batas MAX_WORDS\n",
        "\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Kata ditemukan di FastText, masukkan ke matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"✅ Embedding matrix siap.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGy5nfriZ435",
        "outputId": "155c562a-e532-415e-bc39-0c7bd4000500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📖 Memuat pre-trained word vectors dari cc.id.300.vec...\n",
            "✅ Ditemukan 2000000 word vectors.\n",
            "✅ Embedding matrix siap.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model LSTM + FastText & Training"
      ],
      "metadata": {
        "id": "9XmxlNR5bE6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11.C: Model LSTM + FastText & Training\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping # Import EarlyStopping\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) # Define early_stop\n",
        "\n",
        "# 1. Definisikan Model LSTM + FastText\n",
        "model_lstm_ft = Sequential([\n",
        "    # Layer Embedding (Menggunakan FastText, tidak dilatih)\n",
        "    Embedding(\n",
        "        VOCAB_SIZE,\n",
        "        EMBEDDING_DIM_FT,\n",
        "        weights=[embedding_matrix], # <-- INI BAGIAN PENTING\n",
        "        input_length=LSTM_MAX_LEN,\n",
        "        trainable=False # <-- INI JUGA PENTING\n",
        "    ),\n",
        "\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 2. Compile Model\n",
        "model_lstm_ft.compile(\n",
        "    optimizer=Adam(learning_rate=0.0003),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_lstm_ft.summary()\n",
        "\n",
        "# 3. Training Model\n",
        "print(\"\\nMemulai Training Model LSTM + FastText...\")\n",
        "history_lstm_ft = model_lstm_ft.fit(\n",
        "    X_train_lstm, y_train_lstm, # Data dari Cell 9\n",
        "    epochs=5, # Kita samakan 5 epoch untuk perbandingan\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_lstm, y_val_lstm), # Data dari Cell 9\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "print(\"✅ Training LSTM + FastText selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "beC5L5_ObBmh",
        "outputId": "f5ad8b0a-4e02-4c5e-b34d-b8f8aca66076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_22\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_22\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_24 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │     \u001b[38;5;34m6,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_32 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_43 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_21                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_44 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_45 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_21                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,000,000\u001b[0m (22.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> (22.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,000,000\u001b[0m (22.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> (22.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Memulai Training Model LSTM + FastText...\n",
            "Epoch 1/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6950 - loss: 0.5759 - val_accuracy: 0.8594 - val_loss: 0.3662\n",
            "Epoch 2/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.8672 - loss: 0.3848 - val_accuracy: 0.8631 - val_loss: 0.3535\n",
            "Epoch 3/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.8680 - loss: 0.3689 - val_accuracy: 0.8650 - val_loss: 0.3539\n",
            "Epoch 4/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.8703 - loss: 0.3694 - val_accuracy: 0.8644 - val_loss: 0.3484\n",
            "Epoch 5/5\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.8743 - loss: 0.3585 - val_accuracy: 0.8687 - val_loss: 0.3467\n",
            "✅ Training LSTM + FastText selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi Model LSTM + FastText"
      ],
      "metadata": {
        "id": "e7yrees-arWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11.D: Evaluasi Model LSTM + FastText\n",
        "\n",
        "print(\"Memulai evaluasi final LSTM + FastText pada Test Set...\")\n",
        "\n",
        "# 1. Prediksi Probability\n",
        "y_pred_proba_lstm_ft = model_lstm_ft.predict(X_test_lstm, verbose=0) # Data dari Cell 9\n",
        "\n",
        "# 2. Konversi ke Label Biner (0 atau 1)\n",
        "y_pred_lstm_ft = (y_pred_proba_lstm_ft > 0.5).astype(int)\n",
        "\n",
        "# 3. Hitung Metrik\n",
        "report_lstm_ft = classification_report(y_test_lstm, y_pred_lstm_ft, # y_test_lstm dari Cell 9\n",
        "                                      target_names=['Negative (0)', 'Positive (1)'],\n",
        "                                      output_dict=True, zero_division=0)\n",
        "\n",
        "print(\"\\n--- Hasil Evaluasi Test Set LSTM + FastText ---\")\n",
        "print(classification_report(y_test_lstm, y_pred_lstm_ft,\n",
        "                            target_names=['Negative (0)', 'Positive (1)'],\n",
        "                            zero_division=0))\n",
        "\n",
        "# Ekstrak F1 Macro dan Accuracy untuk perbandingan\n",
        "acc_lstm_ft = report_lstm_ft['accuracy']\n",
        "f1_lstm_ft = report_lstm_ft['macro avg']['f1-score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQIw_QvVayQS",
        "outputId": "0147e614-a65b-40c2-e7fa-094057393a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai evaluasi final LSTM + FastText pada Test Set...\n",
            "\n",
            "--- Hasil Evaluasi Test Set LSTM + FastText ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Negative (0)       0.85      0.94      0.89      1025\n",
            "Positive (1)       0.93      0.83      0.87       975\n",
            "\n",
            "    accuracy                           0.88      2000\n",
            "   macro avg       0.89      0.88      0.88      2000\n",
            "weighted avg       0.89      0.88      0.88      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perbandingan Hasil"
      ],
      "metadata": {
        "id": "dt2CqSiWZg6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 (BARU): Perbandingan Akhir (3 Model)\n",
        "\n",
        "# Impor fungsi untuk \"menggambar\" Markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# 1. Ambil hasil Transformer dari Cell 8\n",
        "acc_trans = test_results_trans.get('eval_accuracy', 0)\n",
        "f1_trans = test_results_trans.get('eval_f1', 0)\n",
        "\n",
        "# 2. Ambil hasil LSTM Biasa dari Cell 11\n",
        "# (Variabel acc_lstm dan f1_lstm sudah ada)\n",
        "\n",
        "# 3. Ambil hasil LSTM + FastText dari Cell 11.D\n",
        "# (Variabel acc_lstm_ft dan f1_lstm_ft sudah ada)\n",
        "\n",
        "# --- INI ADALAH PERBAIKANNYA ---\n",
        "\n",
        "# Buat string tabel dalam format Markdown\n",
        "tabel_md = f\"\"\"\n",
        "--- Hasil Perbandingan Klasifikasi Sentimen ---\n",
        "Dataset: MyIM3 Reviews ({len(df_test)} sampel test)\n",
        "\n",
        "| Model | Accuracy (Test) | F1 Score (Macro) |\n",
        "| :--- | :---: | :---: |\n",
        "| **Transformer (IndoBERT)** | **{acc_trans:.4f}** | **{f1_trans:.4f}** |\n",
        "| **LSTM + FastText (cc.id)** | **{acc_lstm_ft:.4f}** | **{f1_lstm_ft:.4f}** |\n",
        "| **LSTM (From Scratch)** | {acc_lstm:.4f} | {f1_lstm:.4f} |\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Gunakan display(Markdown(...)) untuk mencetak tabel yang rapi\n",
        "display(Markdown(tabel_md))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "YplXd8DWlgIl",
        "outputId": "9efbdbdd-a963-45db-be30-0df2b4bb64a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n--- Hasil Perbandingan Klasifikasi Sentimen ---\nDataset: MyIM3 Reviews (2000 sampel test)\n\n| Model | Accuracy (Test) | F1 Score (Macro) |\n| :--- | :---: | :---: |\n| **Transformer (IndoBERT)** | **0.8910** | **0.8909** |\n| **LSTM + FastText (cc.id)** | **0.8835** | **0.8828** |\n| **LSTM (From Scratch)** | 0.8720 | 0.8719 |\n\n---\n\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}